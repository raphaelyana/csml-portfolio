{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33e79231",
   "metadata": {},
   "source": [
    "## Define Polynomial Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6adf236-c763-4add-b37d-f1641d46e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raphaelyana/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "file_path = '/Users/raphaelyana/Desktop/COMP78 - Supervised Learning/CW2/zipcombo.dat'\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data_lines = file.readlines()\n",
    "\n",
    "    # Split the data into rows and columns\n",
    "    processed_data = [list(map(float, line.strip().split())) for line in data_lines]\n",
    "    df = pd.DataFrame(processed_data)\n",
    "\n",
    "    # Separate labels and features\n",
    "    labels = df.iloc[:, 0].to_numpy()\n",
    "    features = df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "features, labels = load_dataset(file_path)\n",
    "\n",
    "def polynomial_kernel(x, y, degree):\n",
    "    return (np.dot(x, y.T) + 1) ** degree\n",
    "\n",
    "# Optimized kernel perceptron implementation for multi-class\n",
    "class MultiClassKernelPerceptron:\n",
    "    def __init__(self, num_classes, kernel, degree, max_epochs):\n",
    "        self.num_classes = num_classes\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.max_epochs = max_epochs\n",
    "        self.alphas = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        self.alphas = np.zeros((self.num_classes, n_samples))\n",
    "        kernel_matrix = self.kernel(X, X, self.degree)\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            y_binary = np.where(y == c, 1, -1)\n",
    "            for epoch in range(self.max_epochs):\n",
    "                for i in range(n_samples):\n",
    "                    prediction = np.sign(np.dot(self.alphas[c], kernel_matrix[:, i]))\n",
    "                    if prediction != y_binary[i]:\n",
    "                        self.alphas[c, i] += y_binary[i]\n",
    "\n",
    "    def predict(self, X_test, X_train):\n",
    "        test_kernel_matrix = self.kernel(X_test, X_train, self.degree)\n",
    "        scores = np.dot(test_kernel_matrix, self.alphas.T)\n",
    "        return np.argmax(scores, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df4595",
   "metadata": {},
   "source": [
    "## Question 3 Basic results: 20 runs for polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50f61735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree 1, Run 1\n",
      "Degree 1, Run 2\n",
      "Degree 2, Run 1\n",
      "Degree 2, Run 2\n",
      "Degree 3, Run 1\n",
      "Degree 3, Run 2\n",
      "Degree 4, Run 1\n",
      "Degree 4, Run 2\n",
      "Degree 5, Run 1\n",
      "Degree 5, Run 2\n",
      "Degree 6, Run 1\n",
      "Degree 6, Run 2\n",
      "Degree 7, Run 1\n",
      "Degree 7, Run 2\n",
      "\n",
      "Final Results:\n",
      "Degree\tTrain Error (Mean ± Std)\tTest Error (Mean ± Std)\n",
      "1\t0.0843 ± 0.0097\t0.0946 ± 0.0070\n",
      "2\t0.0084 ± 0.0018\t0.0349 ± 0.0070\n",
      "3\t0.0011 ± 0.0002\t0.0272 ± 0.0030\n",
      "4\t0.0008 ± 0.0004\t0.0309 ± 0.0030\n",
      "5\t0.0003 ± 0.0000\t0.0263 ± 0.0005\n",
      "6\t0.0003 ± 0.0001\t0.0301 ± 0.0011\n",
      "7\t0.0003 ± 0.0003\t0.0306 ± 0.0022\n",
      "\n",
      "First 10 Predictions and Original Labels for Each Degree:\n",
      "Degree 1:\n",
      "Original Labels: [1. 9. 2. 2. 8. 9. 4. 5. 7. 1.]\n",
      "Predictions:     [1 9 2 8 8 9 4 5 7 1]\n",
      "Degree 2:\n",
      "Original Labels: [1. 9. 2. 2. 8. 9. 4. 5. 7. 1.]\n",
      "Predictions:     [1 9 2 8 8 9 4 5 7 1]\n",
      "Degree 3:\n",
      "Original Labels: [1. 9. 2. 2. 8. 9. 4. 5. 7. 1.]\n",
      "Predictions:     [1 9 2 1 8 9 4 5 7 1]\n",
      "Degree 4:\n",
      "Original Labels: [1. 9. 2. 2. 8. 9. 4. 5. 7. 1.]\n",
      "Predictions:     [1 9 2 1 8 9 4 5 7 1]\n",
      "Degree 5:\n",
      "Original Labels: [1. 9. 2. 2. 8. 9. 4. 5. 7. 1.]\n",
      "Predictions:     [1 9 2 3 8 9 4 5 7 1]\n",
      "Degree 6:\n",
      "Original Labels: [1. 9. 2. 2. 8. 9. 4. 5. 7. 1.]\n",
      "Predictions:     [1 9 2 3 8 9 4 5 7 1]\n",
      "Degree 7:\n",
      "Original Labels: [1. 9. 2. 2. 8. 9. 4. 5. 7. 1.]\n",
      "Predictions:     [1 9 2 3 8 9 4 5 7 1]\n"
     ]
    }
   ],
   "source": [
    "# Arrays to store results\n",
    "results = []\n",
    "all_predictions = {}\n",
    "all_test_labels = {}\n",
    "\n",
    "max_epochs = 5\n",
    "\n",
    "for degree in range(1, 8):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    first_10_predictions = []\n",
    "    first_10_labels = []\n",
    "\n",
    "    for run in range(20):\n",
    "        print(f\"Degree {degree}, Run {run + 1}\")\n",
    "\n",
    "        # Split dataset into training and testing sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=run)\n",
    "\n",
    "        # Train the perceptron model\n",
    "        num_classes = len(np.unique(train_labels))\n",
    "        perceptron = MultiClassKernelPerceptron(num_classes, polynomial_kernel, degree, max_epochs)\n",
    "        perceptron.train(train_features, train_labels)\n",
    "\n",
    "        # Compute train and test errors\n",
    "        train_predictions = perceptron.predict(train_features, train_features)\n",
    "        test_predictions = perceptron.predict(test_features, train_features)\n",
    "        train_error = np.mean(train_predictions != train_labels)\n",
    "        test_error = np.mean(test_predictions != test_labels)\n",
    "\n",
    "        # Store the first 10 predictions and labels of the first run for each degree\n",
    "        if run == 0:\n",
    "            first_10_predictions.append(test_predictions[:10])\n",
    "            first_10_labels.append(test_labels[:10])\n",
    "\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    # Compute mean and standard deviation for train and test errors\n",
    "    mean_train_error = np.mean(train_errors)\n",
    "    std_train_error = np.std(train_errors)\n",
    "    mean_test_error = np.mean(test_errors)\n",
    "    std_test_error = np.std(test_errors)\n",
    "\n",
    "    results.append((degree, mean_train_error, std_train_error, mean_test_error, std_test_error))\n",
    "    all_predictions[degree] = first_10_predictions[0] if first_10_predictions else []\n",
    "    all_test_labels[degree] = first_10_labels[0] if first_10_labels else []\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Degree\\tTrain Error (Mean ± Std)\\tTest Error (Mean ± Std)\")\n",
    "for degree, mean_train, std_train, mean_test, std_test in results:\n",
    "    print(f\"{degree}\\t{mean_train:.4f} ± {std_train:.4f}\\t{mean_test:.4f} ± {std_test:.4f}\")\n",
    "\n",
    "# Print first 10 predictions and original labels for each degree\n",
    "print(\"\\nFirst 10 Predictions and Original Labels for Each Degree:\")\n",
    "for degree in all_predictions.keys():\n",
    "    print(f\"Degree {degree}:\")\n",
    "    print(f\"Original Labels: {all_test_labels[degree]}\")\n",
    "    print(f\"Predictions:     {all_predictions[degree]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0c22c",
   "metadata": {},
   "source": [
    "## Question 4 & 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcbfbcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Run 1: Best degree = 5, Train Error = 0.0003, Test Error = 0.0258\n",
      "Run 2\n",
      "Run 2: Best degree = 7, Train Error = 0.0005, Test Error = 0.0328\n",
      "Run 3\n",
      "Run 3: Best degree = 5, Train Error = 0.0008, Test Error = 0.0306\n",
      "Run 4\n",
      "Run 4: Best degree = 6, Train Error = 0.0003, Test Error = 0.0285\n",
      "Run 5\n",
      "Run 5: Best degree = 5, Train Error = 0.0003, Test Error = 0.0247\n",
      "Run 6\n",
      "Run 6: Best degree = 5, Train Error = 0.0005, Test Error = 0.0301\n",
      "Run 7\n",
      "Run 7: Best degree = 4, Train Error = 0.0008, Test Error = 0.0323\n",
      "Run 8\n",
      "Run 8: Best degree = 5, Train Error = 0.0004, Test Error = 0.0253\n",
      "Run 9\n",
      "Run 9: Best degree = 6, Train Error = 0.0003, Test Error = 0.0258\n",
      "Run 10\n",
      "Run 10: Best degree = 6, Train Error = 0.0003, Test Error = 0.0301\n",
      "Run 11\n",
      "Run 11: Best degree = 5, Train Error = 0.0007, Test Error = 0.0290\n",
      "Run 12\n",
      "Run 12: Best degree = 6, Train Error = 0.0005, Test Error = 0.0285\n",
      "Run 13\n",
      "Run 13: Best degree = 6, Train Error = 0.0003, Test Error = 0.0306\n",
      "Run 14\n",
      "Run 14: Best degree = 5, Train Error = 0.0005, Test Error = 0.0210\n",
      "Run 15\n",
      "Run 15: Best degree = 4, Train Error = 0.0004, Test Error = 0.0242\n",
      "Run 16\n",
      "Run 16: Best degree = 5, Train Error = 0.0005, Test Error = 0.0231\n",
      "Run 17\n",
      "Run 17: Best degree = 4, Train Error = 0.0008, Test Error = 0.0376\n",
      "Run 18\n",
      "Run 18: Best degree = 5, Train Error = 0.0004, Test Error = 0.0312\n",
      "Run 19\n",
      "Run 19: Best degree = 6, Train Error = 0.0003, Test Error = 0.0301\n",
      "Run 20\n",
      "Run 20: Best degree = 6, Train Error = 0.0005, Test Error = 0.0269\n",
      "\n",
      "Summary of 20 Runs:\n",
      "Mean Train Error: 0.0005 ± 0.0002\n",
      "Mean Test Error: 0.0284 ± 0.0038\n",
      "Mean Best degree: 5.3000 ± 0.7810\n",
      "\n",
      "Averaged Confusion Matrix with Standard Deviations (over 20 runs):\n",
      "0.00% ± 0.00%\t0.07% ± 0.13%\t0.06% ± 0.13%\t0.11% ± 0.18%\t0.05% ± 0.12%\t0.12% ± 0.18%\t0.25% ± 0.21%\t0.05% ± 0.12%\t0.11% ± 0.15%\t0.10% ± 0.15%\t\n",
      "0.00% ± 0.00%\t0.00% ± 0.00%\t0.09% ± 0.23%\t0.04% ± 0.17%\t0.31% ± 0.29%\t0.02% ± 0.08%\t0.17% ± 0.33%\t0.04% ± 0.12%\t0.04% ± 0.11%\t0.06% ± 0.14%\t\n",
      "0.40% ± 0.38%\t0.27% ± 0.41%\t0.00% ± 0.00%\t0.51% ± 0.64%\t0.64% ± 0.50%\t0.16% ± 0.25%\t0.16% ± 0.30%\t0.62% ± 0.46%\t0.19% ± 0.26%\t0.02% ± 0.11%\t\n",
      "0.32% ± 0.29%\t0.30% ± 0.30%\t0.59% ± 0.53%\t0.00% ± 0.00%\t0.18% ± 0.35%\t1.64% ± 1.07%\t0.03% ± 0.14%\t0.33% ± 0.41%\t0.96% ± 0.77%\t0.26% ± 0.46%\t\n",
      "0.26% ± 0.44%\t0.54% ± 0.41%\t0.52% ± 0.79%\t0.03% ± 0.12%\t0.00% ± 0.00%\t0.09% ± 0.21%\t0.58% ± 0.61%\t0.33% ± 0.47%\t0.12% ± 0.30%\t0.93% ± 0.68%\t\n",
      "0.82% ± 0.74%\t0.14% ± 0.28%\t0.36% ± 0.55%\t1.12% ± 0.81%\t0.71% ± 0.91%\t0.00% ± 0.00%\t0.80% ± 0.65%\t0.11% ± 0.25%\t0.45% ± 0.64%\t0.50% ± 0.68%\t\n",
      "0.83% ± 0.80%\t0.22% ± 0.30%\t0.24% ± 0.36%\t0.00% ± 0.00%\t0.40% ± 0.44%\t0.22% ± 0.30%\t0.00% ± 0.00%\t0.00% ± 0.00%\t0.15% ± 0.26%\t0.06% ± 0.19%\t\n",
      "0.10% ± 0.24%\t0.33% ± 0.44%\t0.29% ± 0.38%\t0.16% ± 0.44%\t0.71% ± 0.65%\t0.13% ± 0.25%\t0.00% ± 0.00%\t0.00% ± 0.00%\t0.16% ± 0.27%\t1.23% ± 0.82%\t\n",
      "0.60% ± 0.40%\t0.46% ± 0.55%\t0.50% ± 0.51%\t1.57% ± 1.14%\t0.53% ± 0.49%\t1.20% ± 1.16%\t0.21% ± 0.39%\t0.21% ± 0.32%\t0.00% ± 0.00%\t0.24% ± 0.33%\t\n",
      "0.28% ± 0.46%\t0.09% ± 0.23%\t0.15% ± 0.32%\t0.21% ± 0.29%\t1.35% ± 0.91%\t0.21% ± 0.29%\t0.00% ± 0.00%\t0.94% ± 0.71%\t0.22% ± 0.30%\t0.00% ± 0.00%\t\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "# Parameters\n",
    "degree_values = [1, 2, 3, 4, 5, 6, 7]\n",
    "max_epochs = 5\n",
    "\n",
    "# Arrays to store results\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "best_degrees = []\n",
    "all_confusion_matrices = []\n",
    "\n",
    "for run in range(20):  # Adjust the number of runs as needed\n",
    "    print(f\"Run {run + 1}\")\n",
    "    # Split dataset into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=run)\n",
    "\n",
    "    # Cross-validation setup\n",
    "    kf = KFold(n_splits=5)\n",
    "    best_degree = None\n",
    "    best_cv_error = float('inf')\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for degree in degree_values:\n",
    "        cv_errors = []\n",
    "\n",
    "        for train_index, val_index in kf.split(train_features):\n",
    "            cv_train_features, cv_val_features = train_features[train_index], train_features[val_index]\n",
    "            cv_train_labels, cv_val_labels = train_labels[train_index], train_labels[val_index]\n",
    "\n",
    "            # Train the perceptron model\n",
    "            num_classes = len(np.unique(train_labels))\n",
    "            perceptron = MultiClassKernelPerceptron(num_classes, polynomial_kernel, degree, max_epochs)\n",
    "            perceptron.train(cv_train_features, cv_train_labels)\n",
    "\n",
    "            # Predict on validation set\n",
    "            y_val_pred = perceptron.predict(cv_val_features, cv_train_features)\n",
    "            cv_error = np.mean(y_val_pred != cv_val_labels)\n",
    "            cv_errors.append(cv_error)\n",
    "\n",
    "        # Compute mean CV error for this degree\n",
    "        mean_cv_error = np.mean(cv_errors)\n",
    "        if mean_cv_error < best_cv_error:\n",
    "            best_cv_error = mean_cv_error\n",
    "            best_degree = degree\n",
    "\n",
    "    # Train on the entire training set with the best degree\n",
    "    perceptron = MultiClassKernelPerceptron(num_classes, polynomial_kernel, best_degree, max_epochs)\n",
    "    perceptron.train(train_features, train_labels)\n",
    "\n",
    "    # Compute train and test errors\n",
    "    train_predictions = perceptron.predict(train_features, train_features)\n",
    "    test_predictions = perceptron.predict(test_features, train_features)\n",
    "    train_error = np.mean(train_predictions != train_labels)\n",
    "    test_error = np.mean(test_predictions != test_labels)\n",
    "\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    best_degrees.append(best_degree)\n",
    "\n",
    "    print(f\"Run {run + 1}: Best degree = {best_degree}, Train Error = {train_error:.4f}, Test Error = {test_error:.4f}\")\n",
    "    \n",
    "    test_labels = test_labels.astype(int)\n",
    "    test_predictions = test_predictions.astype(int)\n",
    "    \n",
    "    # Confusion matrix calculation\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "    for true_label, predicted_label in zip(test_labels, test_predictions):\n",
    "        confusion_matrix[true_label, predicted_label] += 1\n",
    "\n",
    "    # Normalize the confusion matrix row-wise\n",
    "    for i in range(10):\n",
    "        total_points = np.sum(confusion_matrix[i, :])\n",
    "        if total_points > 0:\n",
    "            confusion_matrix[i, :] /= total_points\n",
    "    # Set diagonal entries to zero\n",
    "    np.fill_diagonal(confusion_matrix, 0)\n",
    "\n",
    "    all_confusion_matrices.append(confusion_matrix)\n",
    "\n",
    "# Compute mean and standard deviation of confusion matrices\n",
    "all_confusion_matrices = np.array(all_confusion_matrices)\n",
    "mean_confusion_matrix = np.mean(all_confusion_matrices, axis=0)\n",
    "std_confusion_matrix = np.std(all_confusion_matrices, axis=0)\n",
    "\n",
    "# Print summary results\n",
    "print(\"\\nSummary of 20 Runs:\")\n",
    "print(f\"Mean Train Error: {np.mean(train_errors):.4f} ± {np.std(train_errors):.4f}\")\n",
    "print(f\"Mean Test Error: {np.mean(test_errors):.4f} ± {np.std(test_errors):.4f}\")\n",
    "print(f\"Mean Best degree: {np.mean(best_degrees):.4f} ± {np.std(best_degrees):.4f}\")\n",
    "\n",
    "# Print confusion matrix with standard deviations\n",
    "print(\"\\nAveraged Confusion Matrix with Standard Deviations (over 20 runs):\")\n",
    "for i in range(10):\n",
    "    row = \"\"\n",
    "    for j in range(10):\n",
    "        mean_value = mean_confusion_matrix[i, j] * 100  \n",
    "        std_value = std_confusion_matrix[i, j] * 100  \n",
    "        row += f\"{mean_value:.2f}% ± {std_value:.2f}%\\t\"\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad07ae",
   "metadata": {},
   "source": [
    "# Question 6 Hard-to-predict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73007bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA94AAADvCAYAAAAEowy+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlEklEQVR4nO3deVTWZfrH8c/DjoK748aIhpkaZgu5lOWWpmnaXpM1qLk0k1n9tCLTskFGKy1tjmadyWxxsszMstQmQ1NTc8lSJ0fNQ7kM7qS5pMD9+6MjJ9LwvsHbhwffr3P4oy+f7+UFcfHl4vnyPAFjjBEAAAAAAPAiLNgNAAAAAABQlrF4AwAAAADgEYs3AAAAAAAesXgDAAAAAOARizcAAAAAAB6xeAMAAAAA4BGLNwAAAAAAHrF4AwAAAADgEYs3AAAAAAAesXhLCgQCVm8LFiwIdqu/a8+ePXrggQdUr149RUdHq0aNGurSpYv27dtndf4//vEPNWrUSNHR0apfv76eeuopHT9+3HPXKK1CfSYOHjyoQYMGqU6dOoqOjlbDhg31zDPPKC8vz7oGM4FfC/WZqFev3in7vffee61rMBP4tVCfiV/buXOnqlatqkAgoHfffdf6PGYCvxbqM9G3b18lJyerUqVKio2NVcOGDfXwww9rz5491jWYiaJFBLuB0mDp0qWF/js9PV2ZmZn67LPPCh1v0qTJ2WzL2o4dO3TVVVcpIiJCw4cP1/nnn689e/YoMzNTx44dO+35GRkZGj58uNLS0tSpUyetWLFCw4YN0/bt2/Xyyy+fhY8ApU0oz0Rubq46duyojRs3Kj09XQ0bNtTcuXOVlpambdu26YUXXjhtDWYCvxXKM3HClVdeqTFjxhQ6VqNGDatzmQn8VlmYiRPuu+8+xcTEOJ3DTOC3Qn0mDh06pP79+6tBgwaKiYnRypUrlZGRoY8//lhfffWVoqKiijyfmbBgcJLU1FRTvnz50+YOHTp0Fro5vR49epg6deqYffv2OZ+7Z88eExMTY/r371/oeEZGhgkEAmb9+vVnqk2EsFCaibfeestIMjNmzCh0vH///iYsLMxs2LChyPOZCdgIpZkwxpjExETTtWvXYp3LTMBGqM3ECe+++66Ji4szr732mpFkpk+fftpzmAnYCNWZ+LWJEycaSWb+/PlF5pgJO9xqbqlt27ZKTk7W559/riuuuELlypVTnz59JP1ya8mIESNOOqdevXrq1atXoWPZ2dkaMGCAEhISFBUVVXAbRm5ubrH6ysrK0gcffKB+/fqpcuXKzufPnTtXR48eVe/evQsd7927t4wxev/994vVF8q+0joTS5YsUSAQUJcuXQod79atm/Lz8zVz5swiz2cmUFyldSZKiplAcZX2mdi3b5/uu+8+ZWRkqG7dutbnMRMortI+E79VvXp1SVJERNE3STMTdli8Hfzvf//TXXfdpTvvvFMff/yx/vrXvzqdn52drebNm2vevHl64oknNGfOHN1zzz0aNWqU+vXrVyjbq1cvBQIBZWVlFVlz0aJFMsaodu3a+tOf/qS4uDjFxMSobdu2J93ycirr1q2TJDVt2rTQ8Vq1aqlatWoF7wdOpTTOxLFjxxQWFqbIyMhCx6OjoyVJ33zzTZHnMxMoidI4Eyd8/vnnio+PV2RkpJo0aaKxY8daPe8BM4GSKM0zMWjQINWvX18DBw506omZQEmU5pmQfvmTvUOHDmnJkiUaPny4WrdurSuvvLLIc5gJO/yNt4N9+/Zp+vTpat++fbHOHzFihPbv36/169cX/Ga1Q4cOio2N1ZAhQ/Twww8X/N1HeHi4wsPDFQgEiqy5fft2SdKQIUPUrl07zZgxQ4cOHdJTTz2l9u3ba/ny5brooot+9/y9e/cqOjpa5cuXP+l9VapU0d69e4v1seLcUBpnokmTJsrLy9OyZcvUunXrguOLFy+WpNN+TTMTKInSOBOS1LVrV6WkpCgpKUn79+/X9OnTNWTIEK1Zs0ZvvPFGkecyEyiJ0joTH330kd555x2tXr1aYWFuj0MxEyiJ0joTkrRs2TK1atWq4L+vu+46TZs2TeHh4UWex0zY4RFvB5UrVy72kEjS7Nmz1a5dO9WuXVu5ubkFbyduiV24cGFB9pVXXlFubq4SExOLrJmfny9JSkhI0IwZM3Tttdfqpptu0ty5cxUWFqZnnnnmtH0VNYy2g4pzU2mciZ49e6pKlSrq37+/li9frpycHL311lsFT6pm8wMWM4HiKo0zIUkTJkxQ7969dfXVV6tHjx568803NXDgQL355pv66quvTns+M4HiKo0z8eOPP2rAgAF69NFHlZycXKy+mAkUV2mciROaNm2qFStWaOHChRo/fry++uordezYUYcPHz7tuczE6bF4O6hVq1aJzt+5c6c+/PBDRUZGFnq78MILJcnp6fpPqFq1qiTpmmuuKfTbqFq1aqlZs2ZavXr1ac8/evToKQdq3759qlKlinNPOHeUxpmoVq2a5s6dK0lq2bKlKleurPvvv1/PPfecJKlOnTpFns9MoCRK40z8nrvuukvSL49wFIWZQEmUxpl4/PHHFRkZqYEDByonJ0c5OTn66aefJEmHDx9WTk6OjDG/ez4zgZIojTNxQvny5ZWSkqKrr75agwYN0syZM7V8+XK99NJLRZ7HTNjhVnMHv/fbmujoaP38888nHf/tbRXVqlXTRRddpIyMjFPWqV27tnNPRd1Gbow57aN7J/4WY+3atWrRokXB8ezsbO3Zs6fYvwnGuaE0zoQkXX755frPf/6jrKwsHTp0SOeff75WrVolSbr66quLPJeZQEmU1pk4lROLBdcJ+FQaZ2LdunXKyspSzZo1T3pfamqqJGn//v2qVKnSKc9nJlASpXEmfk9KSorCwsK0cePGInPMhB0W7zOgXr16Jz1h02effVbw29MTunXrpo8//lhJSUnFegbyU2nRooUSEhL0ySefKC8vr+BR7x07dujrr7/WnXfeWeT5nTt3VkxMjKZMmVJoUKZMmaJAIKAbbrjhjPSJc0swZ+K3fUi/LBhjx45V7dq1deuttxZ5DjMBH0rLTPza66+/LumXO0OKwkzAh2DOxLhx45STk1Po2Jo1a/TQQw9pxIgRatOmjeLi4n73fGYCPpTG68TChQuVn5+vBg0aFJljJiwF63XMSrNTve5emzZtzIUXXnjK/MiRI00gEDDDhw83n376qXnhhRdMw4YNTcWKFU1qampBbseOHSYxMdE0atTITJw40cyfP9989NFHZsKECaZr165m69atBdk+ffqY8PBwk5WVddp+p0+fbgKBgOnatauZPXu2efvtt01ycrKpWLGi2bx5c0FuwYIFJjw83Dz11FOn7H/o0KFmwYIF5tlnnzXR0dGmX79+Np8unANCbSaGDh1q3nrrLbNgwQLz+uuvm7Zt25rY2Fjz2WefFcoxEyiuUJqJqVOnmptvvtlMnjzZzJ8/38yYMcPccccdRpLp1atXoSwzgeIKpZk4lczMzFO+jjczgeIKpZn48MMPTffu3c0///lP8+9//9t8/PHH5m9/+5upUqWKadCggcnJySnIMhPFxyPeZ8DDDz+sAwcOaMqUKRozZoyaN2+ud955Rz169CiUq1WrllauXKn09HQ9++yz2rZtm+Lj41W/fn117ty50G+t8vLylJeXV+TfGJ1wyy23aObMmcrIyNAtt9yi6OhotWnTRm+//baSkpIKcsYY5eXlFTwh2wmPP/644uPjNWHCBI0ZM0Y1a9ZUWlqaHn/88RJ+ZnCuCvZM7N+/X48++qiys7NVoUIFtWnTRsuXLz/pZS6YCZwtwZyJ8847Tzk5ORo6dKj27t1b8LeAEydO1IABAwplmQmcLcG+TthiJnC2BHMmGjRooKioKKWnp2vnzp2SfnkE/p577lFaWpoqVqxYkGUmii9gzuR3JwAAAAAAUAjPag4AAAAAgEcs3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOJt4cRr0J14i4iIUEJCgnr37q3t27eflR7q1aunXr16Ffv8YcOGqVu3bqpTp44CgUCJagGhPhMjRowo1P9v36ZNm3Zmm0WZF+ozIUmbN2/W3Xffrbp16yo2NlZJSUn6v//7P+3du/fMNYlzRlmYiY0bN+rmm29W5cqVVa5cObVo0UIffPDBmWsQ55SyMBNcJ0qGlxNz8Oqrr6pRo0Y6cuSIPv/8c40aNUoLFy7U2rVrVb58+WC3V6Tnn39eF110kbp3767JkycHux2UEaE6E3379lXnzp1POt6vXz999913p3wfYCNUZ2L37t1q2bKlKlSooPT0dNWtW1dfffWVnnzySWVmZmrVqlUKC+N39XAXqjORlZWlVq1aqVatWpo0aZLi4uL04osv6oYbbtD06dN18803B7tFhKhQnQmuEyXH4u0gOTlZKSkpkqR27dopLy9P6enpev/999WzZ89TnnP48GGVK1fubLZ5SgcPHiwYhjfeeCPI3aCsCNWZSEhIUEJCQqFjWVlZWr9+vXr27KlKlSoFpzGEvFCdiVmzZmnv3r16++231aFDB0m/9P/zzz9r6NCh+vrrr3XJJZcEtUeEplCdidGjR+vw4cOaN2+e6tSpI0nq3LmzmjZtqoceekg33ngjSwaKJVRngutEyfEdowRatmwpSfr+++8lSb169VJcXJzWrl2rTp06KT4+vuAL89ixYxo5cqQaNWqk6OhoVa9eXb1799bu3bsL1Tx+/LgeeeQR1axZU+XKlVPr1q315ZdflrhXLg44G0JpJn5r8uTJMsaob9++Z7w2zl2hMhORkZGSpIoVKxY6fuKXUDExMSWqD5wQKjOxZMkSNWvWrGDplqTw8HB16dJFW7du9XIdwrkpVGaC60TJ8Yh3CWzevFmSVL169YJjx44dU/fu3TVgwAClpaUpNzdX+fn56tGjhxYtWqRHHnlEV1xxhb7//ns9+eSTatu2rVauXKnY2FhJv9zq+vrrr2vIkCHq2LGj1q1bp5tuukkHDx486d+vV6+epF8eqQNKg1Cdifz8fE2ZMkUNGjRQmzZtivfBA6cQKjNxww03qG7duho8eLAmTpyoxMRErV69WqNHj9b111+vxo0bn5lPCM55oTITx44dU5UqVU46Hh0dLUn65ptvChYmoCRCZSa4TpwBBqf16quvGklm2bJl5vjx4+bgwYNm9uzZpnr16iY+Pt5kZ2cbY4xJTU01kszkyZMLnf/WW28ZSWbGjBmFjq9YscJIMhMnTjTGGPPtt98aSeahhx4qlJs6daqRZFJTUwsdT0pKMklJSc4fT/ny5U+qBbgoazMxZ84cI8mMGjXK+VzAmLIxEzt27DCtWrUykgrebr31VnP06FGXTwVgjAn9mbjhhhtMpUqVzMGDBwsdv+qqq4wk8/e//93q8wCcEOozYQzXiZLi/mMHLVu2VGRkpOLj49WtWzfVrFlTc+bMUY0aNQrlfvuEG7Nnz1alSpV0/fXXKzc3t+Dt4osvVs2aNbVgwQJJUmZmpiSd9Pcdt912myIiTr45YfPmzQW/JQOCoazMxCuvvKKIiAie7R8lFqozsX//fvXo0UMHDhzQ1KlT9fnnn2vixIlavHixunfvrtzcXJdPA1AgVGdi4MCB+vHHH/XnP/9ZW7Zs0c6dOzV8+HB98cUXkvgTPhRfqM4E14mS41ZzB6+//roaN26siIgI1ahRQ7Vq1TopU65cOVWoUKHQsZ07dyonJ0dRUVGnrLtnzx5JKngq/po1axZ6f0REhKpWrXomPgTgjCoLM7Fnzx598MEH6tq160n/DuAqVGfi6aef1po1a/T9998X9HzVVVepUaNGat++vaZOnarU1NRi18e5K1RnokOHDnr11Vc1ePBgJSUlSZKaNGmi9PR0DR06tNDffgMuQnUmuE6UHIu3g8aNGxc8C+HvCQQCJx2rVq2aqlatqrlz557ynPj4eEkqGIbs7OxC39Bzc3N5fTyUSmVhJt544w0dO3aMJ1XDGRGqM7FmzRrVqVPnpB8AL7/8cknSunXril0b57ZQnQlJSk1NVc+ePbVp0yZFRkaqQYMGGjVqlAKBgK666qoS1ca5K1RngutEybF4nwXdunXTtGnTlJeXpxYtWvxurm3btpKkqVOn6rLLLis4/s4773D7BsqU0jQTr7zyimrXrq0uXbqckXpAcQR7JmrXrq358+dr+/bthX5QW7p0qSSd9PJ7gG/BnokTIiIiCp406scff9TLL7+sHj16KDExscS1ARfBngmuEyXH4n0W3HHHHZo6daquu+46PfDAA2revLkiIyO1bds2ZWZmqkePHrrxxhvVuHFj3XXXXRo3bpwiIyN1zTXXaN26dRozZsxJt5tIUoMGDSTJ6u8yFi5cWPBSA3l5efr+++/17rvvSpLatGlT6JkUAd9Kw0xI0vLly7V+/XoNHTpU4eHhZ/RjBFwEeybuu+8+TZ06VR07dlRaWpr++Mc/at26dRo5cqRq1Kjxu68tC/gS7JnYtWuXxo4dqyuvvFLx8fHasGGDnnnmGYWFhWnChAlePmagKMGeCa4TZ0Cwn90tFJx4FsIVK1YUmUtNTTXly5c/5fuOHz9uxowZY5o1a2ZiYmJMXFycadSokRkwYIDZtGlTQe7nn382gwcPNn/4wx9MTEyMadmypVm6dKlJTEw86VkIExMTTWJiotXH0KZNm0LPQPjrt8zMTKsawAllYSaMMaZfv34mEAiY7777zvoc4FTKwkysXr3a3HjjjSYhIcFER0eb8847z/Tt29f88MMPVucDvxbqM7F3717TqVMnU716dRMZGWnq1q1r7r//frN79+7TngucSqjPhDFcJ0oqYIwxQdr5AQAAAAAo83gtBAAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPIqwDQYCAZ99hJxPP/3UKR8fH2+dveKKK6yzeXl5Tn2gsJK8jL3PmejQoYN1ds6cOdbZnTt3OvWxceNG6+ySJUucai9evNg6O3/+fOssM1EypXUmSot69eo55WNiYqyzjRo18tZHeHi4dbZjx45OtaOioqyz7dq1c6p90003WWdnzpzpVNtFcefiXJgJn9LT062zo0ePts7efPPNTn306tXLOtu1a1fr7JEjR5z6KC3OxetEly5drLPTpk1zql2hQgXrbHZ2tnV2/PjxTn08++yz1ll+1jqZzVzwiDcAAAAAAB6xeAMAAAAA4BGLNwAAAAAAHrF4AwAAAADgEYs3AAAAAAAesXgDAAAAAOARizcAAAAAAB6xeAMAAAAA4BGLNwAAAAAAHkUEuwEAAIKpe/fu1tnp06c71Y6KinJtB78SExMT7BZwBlWqVMkpP2zYMOts1apVrbPZ2dlOfVxyySXW2SeeeMI6+9hjjzn1geBp2LChdbZChQre+qhZs6Z1dtSoUU61r732Wuvs7bff7lR7165dTvmyisW7mCZNmuSUd/lhrX79+tbZzZs3O/WB0ODyDT4yMtI6m5WV5dTH6tWrrbNt2rRxqp2Wlmadzc/Pt87Onz/fqY/nn3/eOpuZmWmdNcY49eHyMQIAACC0cKs5AAAAAAAesXgDAAAAAOARizcAAAAAAB6xeAMAAAAA4BGLNwAAAAAAHrF4AwAAAADgEYs3AAAAAAAesXgDAAAAAOARizcAAAAAAB6xeAMAAAAA4FFEsBsAAOBMioqKcspnZGR4q11a/PTTT9bZb775xqn2ggULrLPz5s1zqr1s2TKnPEq3hIQEp/y6deuss0eOHLHO3n333U59bNmyxTr72GOPOdVGaBg/frx11hjjVLtu3brW2X/961/W2QsuuMCpjzfeeMNLH5J0zTXXOOXLKhbvYtqwYYO32i4XhCeffNJbHwieOXPmWGdzc3Ots6+88opTH1OmTHHKu4iPj7fOtm3b1jrbuHFjpz4mTZpknY2OjrbOHjx40KmPW2+91Tq7fv16p9oAAAAILm41BwAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMCjiGA3AADAmXTRRRc55ZOTkz114mbt2rXW2eeee86p9qxZs6yz+/fvd6oN2EpJSXHKr1mzxjo7btw462xqaqpTH9nZ2U55nNteeOGFYLcgSdqwYYNTfteuXdbZDh06ONVu1qyZdfbrr792qh1KWLyLqUaNGt5qX3rppd5qIzRkZWVZZ4cPH26dnTRpklMfN954o3X2nXfecap98OBB62xUVJR11nU2Dx8+bJ1NSkpyqu3isssus86uX7/eWx8AAAA487jVHAAAAAAAj1i8AQAAAADwiMUbAAAAAACPWLwBAAAAAPCIxRsAAAAAAI9YvAEAAAAA8IjFGwAAAAAAj1i8AQAAAADwiMUbAAAAAACPWLwBAAAAAPAoItgNhKpKlSp5q52Tk+OtNsqe0aNHW2c//fRTp9p9+/a1zqanpzvVrlOnjnU2KirKOnvgwAGnPmJjY53ytlzn+L333vPSx7lo5MiRwW6hwOrVq62z7du3t87++OOPxWkHOOPCwuwfw+ndu7dT7e3bt1tnu3fvbp3dsmWLUx8//PCDUx7wxeVnp6FDhzrVrlWrlms7Xmp//fXX3voINh7xBgAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADyKCHYDAACcToUKFayzzZs399iJm6ioKOvsokWLrLMbNmxw6mPVqlXW2Xnz5jnVXrNmjVMeZcvgwYOts66z+dlnn1lnL774Yuts1apVnfrIzMx0yqPsufDCC62znTt3dqrdokULL7Xj4+Od+nCxa9cup/zKlSs9dRJaWLyL6fjx495qu/ygBrhw/cbn8xtl7dq1rbOVK1e2zm7bts2pj0mTJlln77jjDuvs9OnTnfr46aefnPIAAAAIHdxqDgAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcs3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHgUEewGQlXr1q291d61a5e32kBpsWPHDi/ZSy+91KmP6667zilva/LkyV7qnquqVKlina1cubLHTtwkJyd7qdu0aVOn/K233mqdHTZsmFPtLl26WGcXL17sVBtnX0pKilM+IyPDOtutWzen2p988olT3tZ7773nlM/KyvLSB0LH2LFjrbPXXnutx078WblypXW2a9euTrX37Nnj2k6ZxCPeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcs3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcRwW4AAIDTady4sbfa2dnZ1tmVK1c61a5bt6519r///a91tmXLlk59/PGPf7TOxsXFOdWeOHGidTYlJcWp9rFjx5zyKLmOHTs65UeOHGmd/eSTT1zb8aJChQpO+ZycHD+NIGQ8//zz1lnX788VK1Z0bceLCy64wDp74YUXOtXetWuXaztlEot3MTVr1sxb7cWLF3urDYSi8PBw6+z48eOdarv8ADZ58mTr7PLly536AAAAQNnFreYAAAAAAHjE4g0AAAAAgEcs3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcs3gAAAAAAeBQR7AYAADidpUuXWmc7d+7sVHvJkiXW2Ysvvtip9rJly6yzubm51tmoqCinPl577TXr7B133OFUu3HjxtbZunXrOtXevHmzUx4lN2rUqGC34F21atWc8vv37/fUCULFvHnzrLNjxoxxqp2dnW2dXbx4sXU2JSXFqY/x48dbZ6dNm+ZUOyEhwTp7/Phxp9qhhMX7VypXrmyddf1izsvLs86uWrXKqTZQ1t12223W2datW3vr48UXX7TOGmO89QEAAIDQwq3mAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcs3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcRwW6gNOnZs6d1tkqVKk61t23b5tqOlXvvvdcpHxsba5196aWXrLOHDx926gOIj4+3zo4cOdJbHy+//LJ1dtWqVd76QNFycnKss/PmzXOqnZycbJ1dtGiRU+1Zs2ZZZ/v372+dTUtLc+qjR48eTnkXmzZtss5u3rzZWx+ArcqVKzvld+/e7akThIqICPuVaeDAgU61Xb4emzZtap198803nfpISUmxzj7wwANOtbt162adnTlzplPtUMIj3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcs3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHEcFuAACAYHr00Ue91e7evbt1tn379tbZ+Pj44rRjxRjjlB88eLCnTgA/ypUr55Tfvn27p04QKpKTk62z1apVc6o9efJk6+ymTZucartITEz0VrtPnz7W2ZkzZ3rrI9hYvH+lb9++3mrXqlXLOrtx40brbCAQKE47Vm6//XbrbKdOnZxqHzhwwLUdlDEjRoywzp533nnW2d27dzv18fDDD1tnXRcSAAAAQOJWcwAAAAAAvGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMCjiGA34FPv3r2d8s2aNfPUiRQeHu6l7rfffuuUX7t2rXX2tttus85edtllTn1kZmY65VH6XXrppU75Bx980Esf999/v1P+wIEDXvpA6Chfvry32oFAwDobHx9vnTXGOPUxYcIE6+wnn3ziVHvOnDlOecCHmJgY6+z27dudamdnZ7u2gzKmadOm1lnXn/lzcnKssy7f+8877zynPjp16uSUd7F//35vtUMJj3gDAAAAAOARizcAAAAAAB6xeAMAAAAA4BGLNwAAAAAAHrF4AwAAAADgEYs3AAAAAAAesXgDAAAAAOARizcAAAAAAB6xeAMAAAAA4BGLNwAAAAAAHkUEuwFXUVFR1tkHH3zQWx/z5s1zynfq1Mk6W79+fevs1q1bnfro06ePdfa2226zzlasWNGpD4QGl/+vr776qlPtsDD73/t98MEH1tn333/fqQ9g4MCB1tmlS5c61e7fv7919tChQ9bZnJwcpz7Gjh1rnc3KynKqDZQG9erVs85u2bLFXyMok5KSkrzVdvlZKy4uzjo7btw4pz7KlStnnT1y5IhT7YyMDKd8WcUj3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHjE4g0AAAAAgEcs3gAAAAAAeMTiDQAAAACARyzeAAAAAAB4xOINAAAAAIBHLN4AAAAAAHgUMMYYq2Ag4LsXKw0aNLDObtq0yal2fn6+dbZJkyZOtWfNmmWdffrpp62z4eHhTn2MHz/eOrt8+XLr7LXXXuvUx/Hjx53yvlh++Z9SaZkJn/r162edffnll51qb9myxTrbqlUr6+yuXbuc+kBhzMSZlZCQYJ1ds2aNdbZq1apOfaxatco6O2LECKfas2fPdsqHouLOBTNx9tx+++3W2fr16zvVHj16tGs7Zdq5eJ3o3LmzdXbOnDlOtX/66Sfr7N69e62ziYmJTn24SE9Pd8o/8cQTnjopPWzmgke8AQAAAADwiMUbAAAAAACPWLwBAAAAAPCIxRsAAAAAAI9YvAEAAAAA8IjFGwAAAAAAj1i8AQAAAADwiMUbAAAAAACPWLwBAAAAAPCIxRsAAAAAAI8CxhhjFQwEfPdi5YorrrDOLlmyxFsfX375pVP+sssus86Gh4e7tmMtMzPTOnvLLbdYZ/ft21ecdoLO8sv/lErLTLho1qyZU3727NnW2YSEBKfa7du3t866fN2iZM61mfDN5XPicl1JSUkpTjtW8vPznfKxsbHW2WPHjrm2UyoUdy6YibPnL3/5i3V2x44dTrVnzZrl2k6Zdi5eJ8LC7B+rzMjIcKqdlpbm2o4XN910k3V2zpw5TrWPHj3q2k7IsZkLHvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPGLxBgAAAADAo4hgN+Dqiy++sM4OGzbMqfagQYOss82bN3eq7WLbtm3W2UceecSp9rRp06yzxhin2ij9YmNjnfIuXwPjxo1zqp2ZmemUB0KRywz169fPOvv+++879bFhwwbr7IoVK5xqHz9+3CkP+JCbm2ud3bp1q8dOUBbl5+dbZ4cOHepUOy4uzjo7cOBA62xaWppTHy7XFXaE4uERbwAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMCjgDHGBLsJAAAAAADKKh7xBgAAAADAIxZvAAAAAAA8YvEGAAAAAMAjFm8AAAAAADxi8QYAAAAAwCMWbwAAAAAAPGLxBgAAAADAIxZvAAAAAAA8YvEGAAAAAMCj/wecx2Vtio6aOAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_hard_to_predict_samples(hard_samples, true_labels, predicted_labels):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i, (image, true_label, predicted_label) in enumerate(zip(hard_samples, true_labels, predicted_labels)):\n",
    "        plt.subplot(1, 5, i + 1)\n",
    "        plt.imshow(image.reshape(16, 16), cmap=\"gray\")  # Assuming images are 16x16 pixels\n",
    "        plt.title(f\"True: {true_label}\\nPred: {predicted_label}\")\n",
    "        plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Parameters\n",
    "degree = 5  # Choose an optimal degree for the polynomial kernel\n",
    "max_epochs = 5\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the perceptron model\n",
    "num_classes = len(np.unique(train_labels))\n",
    "perceptron = MultiClassKernelPerceptron(num_classes, polynomial_kernel, degree, max_epochs)\n",
    "perceptron.train(train_features, train_labels)\n",
    "\n",
    "# Predict on the test set\n",
    "test_predictions = perceptron.predict(test_features, train_features)\n",
    "\n",
    "# Find misclassified samples\n",
    "misclassified_indices = np.where(test_predictions != test_labels)[0]\n",
    "misclassified_images = test_features[misclassified_indices]\n",
    "misclassified_true_labels = test_labels[misclassified_indices]\n",
    "misclassified_predicted_labels = test_predictions[misclassified_indices]\n",
    "\n",
    "# Select the first five misclassified samples for visualization\n",
    "hard_to_predict_samples = misclassified_images[:5]\n",
    "hard_to_predict_true_labels = misclassified_true_labels[:5]\n",
    "hard_to_predict_predicted_labels = misclassified_predicted_labels[:5]\n",
    "\n",
    "# Plot the hard-to-predict samples\n",
    "plot_hard_to_predict_samples(hard_to_predict_samples, hard_to_predict_true_labels, hard_to_predict_predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e29035",
   "metadata": {},
   "source": [
    "# Question 7 Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7555828",
   "metadata": {},
   "source": [
    "### Define gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda8d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, c):\n",
    "    dist_sq = np.sum(x ** 2, axis=1, keepdims=True) + np.sum(y ** 2, axis=1) - 2 * np.dot(x, y.T)\n",
    "    return np.exp(-c * dist_sq)\n",
    "\n",
    "# Optimized kernel perceptron implementation for multi-class\n",
    "class MultiClassKernelPerceptron:\n",
    "    def __init__(self, num_classes, kernel, c, max_epochs):\n",
    "        self.num_classes = num_classes\n",
    "        self.kernel = kernel\n",
    "        self.c = c\n",
    "        self.max_epochs = max_epochs\n",
    "        self.alphas = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        self.alphas = np.zeros((self.num_classes, n_samples))\n",
    "        kernel_matrix = self.kernel(X, X, self.c)\n",
    "\n",
    "        for c in range(self.num_classes):\n",
    "            y_binary = np.where(y == c, 1, -1)\n",
    "            for epoch in range(self.max_epochs):\n",
    "                for i in range(n_samples):\n",
    "                    prediction = np.sign(np.dot(self.alphas[c], kernel_matrix[:, i]))\n",
    "                    if prediction != y_binary[i]:\n",
    "                        self.alphas[c, i] += y_binary[i]\n",
    "\n",
    "    def predict(self, X_test, X_train):\n",
    "        test_kernel_matrix = self.kernel(X_test, X_train, self.c)\n",
    "        scores = np.dot(test_kernel_matrix, self.alphas.T)\n",
    "        return np.argmax(scores, axis=1)\n",
    "\n",
    "# Set up the parameters\n",
    "max_epochs = 5\n",
    "num_classes = len(np.unique(labels))\n",
    "c_values = [0.01, 0.05,0.1,0.5, 1, 2, 5, 10]  \n",
    "results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f660812d",
   "metadata": {},
   "source": [
    "### (b) perform 20 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b675480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for c = 0.01\n",
      "c = 0.01: Train Error = 0.0005646679 ± 0.0001929645, Test Error = 0.0283064516 ± 0.0038480317\n",
      "Evaluating for c = 0.05\n",
      "c = 0.05: Train Error = 0.0002016671 ± 0.0002460743, Test Error = 0.0394892473 ± 0.0036759217\n",
      "Evaluating for c = 0.1\n",
      "c = 0.1: Train Error = 0.0001747782 ± 0.0002172021, Test Error = 0.0533602151 ± 0.0062232779\n",
      "Evaluating for c = 0.5\n",
      "c = 0.5: Train Error = 0.0000000000 ± 0.0000000000, Test Error = 0.0681182796 ± 0.0056004373\n",
      "Evaluating for c = 1\n",
      "c = 1: Train Error = 0.0000000000 ± 0.0000000000, Test Error = 0.0680107527 ± 0.0053937889\n",
      "Evaluating for c = 2\n",
      "c = 2: Train Error = 0.0000000000 ± 0.0000000000, Test Error = 0.0684139785 ± 0.0051732472\n",
      "Evaluating for c = 5\n",
      "c = 5: Train Error = 0.0000067222 ± 0.0000293016, Test Error = 0.0685215054 ± 0.0063305738\n",
      "Evaluating for c = 10\n",
      "c = 10: Train Error = 0.0000000000 ± 0.0000000000, Test Error = 0.1952150538 ± 0.0071606364\n",
      "\n",
      "Final Results:\n",
      "c\tTrain Error (Mean ± Std)\tTest Error (Mean ± Std)\n",
      "0.01\t0.0005646679 ± 0.0001929645\t\t0.0283064516 ± 0.0038480317\n",
      "0.05\t0.0002016671 ± 0.0002460743\t\t0.0394892473 ± 0.0036759217\n",
      "0.1\t0.0001747782 ± 0.0002172021\t\t0.0533602151 ± 0.0062232779\n",
      "0.5\t0.0000000000 ± 0.0000000000\t\t0.0681182796 ± 0.0056004373\n",
      "1\t0.0000000000 ± 0.0000000000\t\t0.0680107527 ± 0.0053937889\n",
      "2\t0.0000000000 ± 0.0000000000\t\t0.0684139785 ± 0.0051732472\n",
      "5\t0.0000067222 ± 0.0000293016\t\t0.0685215054 ± 0.0063305738\n",
      "10\t0.0000000000 ± 0.0000000000\t\t0.1952150538 ± 0.0071606364\n"
     ]
    }
   ],
   "source": [
    "# Loop through each c value\n",
    "for c in c_values:\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    print(f\"Evaluating for c = {c}\")\n",
    "    \n",
    "    # Perform 20 runs\n",
    "    for run in range(20):\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features, labels, test_size=0.2, random_state=run\n",
    "        )\n",
    "\n",
    "        # Train the perceptron model\n",
    "        num_classes = len(np.unique(train_labels))\n",
    "        perceptron = MultiClassKernelPerceptron(num_classes, gaussian_kernel, c, max_epochs)\n",
    "        perceptron.train(train_features, train_labels)\n",
    "\n",
    "        # Predict on training and testing sets\n",
    "        y_train_pred = perceptron.predict(train_features, train_features)\n",
    "        y_test_pred = perceptron.predict(test_features, train_features)\n",
    "\n",
    "        # Calculate train and test errors\n",
    "        train_error = np.mean(y_train_pred != train_labels)\n",
    "        test_error = np.mean(y_test_pred != test_labels)\n",
    "\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    # Compute mean and std for this c value\n",
    "    mean_train_error = np.mean(train_errors)\n",
    "    std_train_error = np.std(train_errors)\n",
    "    mean_test_error = np.mean(test_errors)\n",
    "    std_test_error = np.std(test_errors)\n",
    "\n",
    "    results.append((c, mean_train_error, std_train_error, mean_test_error, std_test_error))\n",
    "\n",
    "    print(f\"c = {c}: Train Error = {mean_train_error:.10f} ± {std_train_error:.10f}, \"\n",
    "          f\"Test Error = {mean_test_error:.10f} ± {std_test_error:.10f}\")\n",
    "\n",
    "# Display results in a formatted way\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"c\\tTrain Error (Mean ± Std)\\tTest Error (Mean ± Std)\")\n",
    "for c, mean_train, std_train, mean_test, std_test in results:\n",
    "    print(f\"{c}\\t{mean_train:.10f} ± {std_train:.10f}\\t\\t{mean_test:.10f} ± {std_test:.10f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd65c1f0",
   "metadata": {},
   "source": [
    "### (c) cross-validation to find best c and report train and test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bacfa82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Run 1: Best c = 0.01, Train Error = 0.0008, Test Error = 0.0274\n",
      "Run 2\n",
      "Run 2: Best c = 0.01, Train Error = 0.0007, Test Error = 0.0258\n",
      "Run 3\n",
      "Run 3: Best c = 0.01, Train Error = 0.0007, Test Error = 0.0323\n",
      "Run 4\n",
      "Run 4: Best c = 0.01, Train Error = 0.0005, Test Error = 0.0323\n",
      "Run 5\n",
      "Run 5: Best c = 0.01, Train Error = 0.0003, Test Error = 0.0269\n",
      "Run 6\n",
      "Run 6: Best c = 0.01, Train Error = 0.0004, Test Error = 0.0306\n",
      "Run 7\n",
      "Run 7: Best c = 0.01, Train Error = 0.0008, Test Error = 0.0269\n",
      "Run 8\n",
      "Run 8: Best c = 0.01, Train Error = 0.0008, Test Error = 0.0285\n",
      "Run 9\n",
      "Run 9: Best c = 0.01, Train Error = 0.0003, Test Error = 0.0220\n",
      "Run 10\n",
      "Run 10: Best c = 0.01, Train Error = 0.0005, Test Error = 0.0328\n",
      "Run 11\n",
      "Run 11: Best c = 0.01, Train Error = 0.0004, Test Error = 0.0306\n",
      "Run 12\n",
      "Run 12: Best c = 0.01, Train Error = 0.0008, Test Error = 0.0269\n",
      "Run 13\n",
      "Run 13: Best c = 0.01, Train Error = 0.0008, Test Error = 0.0317\n",
      "Run 14\n",
      "Run 14: Best c = 0.01, Train Error = 0.0008, Test Error = 0.0247\n",
      "Run 15\n",
      "Run 15: Best c = 0.01, Train Error = 0.0004, Test Error = 0.0242\n",
      "Run 16\n",
      "Run 16: Best c = 0.01, Train Error = 0.0005, Test Error = 0.0226\n",
      "Run 17\n",
      "Run 17: Best c = 0.01, Train Error = 0.0004, Test Error = 0.0376\n",
      "Run 18\n",
      "Run 18: Best c = 0.01, Train Error = 0.0005, Test Error = 0.0247\n",
      "Run 19\n",
      "Run 19: Best c = 0.01, Train Error = 0.0005, Test Error = 0.0306\n",
      "Run 20\n",
      "Run 20: Best c = 0.01, Train Error = 0.0003, Test Error = 0.0269\n",
      "\n",
      "Summary of 20 Runs:\n",
      "Mean Train Error: 0.0006 ± 0.0002\n",
      "Mean Test Error: 0.0283 ± 0.0038\n",
      "Mean Best c: 0.0100 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Arrays to store results\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "best_c_values = []\n",
    "\n",
    "for run in range(20):\n",
    "    print(f\"Run {run + 1}\")\n",
    "    # Split dataset into training and testing sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2, random_state=run)\n",
    "\n",
    "    # Cross-validation setup\n",
    "    kf = KFold(n_splits=5)\n",
    "    best_c = None\n",
    "    best_cv_error = float('inf')\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for c in c_values:\n",
    "        cv_errors = []\n",
    "\n",
    "        for train_index, val_index in kf.split(train_features):\n",
    "            cv_train_features, cv_val_features = train_features[train_index], train_features[val_index]\n",
    "            cv_train_labels, cv_val_labels = train_labels[train_index], train_labels[val_index]\n",
    "\n",
    "            # Train the perceptron model\n",
    "            num_classes = len(np.unique(train_labels))\n",
    "            perceptron = MultiClassKernelPerceptron(num_classes, gaussian_kernel, c, max_epochs)\n",
    "            perceptron.train(cv_train_features, cv_train_labels)\n",
    "\n",
    "            # Predict on validation set\n",
    "            y_val_pred = perceptron.predict(cv_val_features, cv_train_features)\n",
    "            cv_error = np.mean(y_val_pred != cv_val_labels)\n",
    "            cv_errors.append(cv_error)\n",
    "\n",
    "        # Compute mean CV error for this c value\n",
    "        mean_cv_error = np.mean(cv_errors)\n",
    "        if mean_cv_error < best_cv_error:\n",
    "            best_cv_error = mean_cv_error\n",
    "            best_c = c\n",
    "\n",
    "    # Train on the entire training set with the best c value\n",
    "    perceptron = MultiClassKernelPerceptron(num_classes, gaussian_kernel, best_c, max_epochs)\n",
    "    perceptron.train(train_features, train_labels)\n",
    "\n",
    "    # Compute train and test errors\n",
    "    train_predictions = perceptron.predict(train_features, train_features)\n",
    "    test_predictions = perceptron.predict(test_features, train_features)\n",
    "    train_error = np.mean(train_predictions != train_labels)\n",
    "    test_error = np.mean(test_predictions != test_labels)\n",
    "\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_error)\n",
    "    best_c_values.append(best_c)\n",
    "\n",
    "    print(f\"Run {run + 1}: Best c = {best_c}, Train Error = {train_error:.4f}, Test Error = {test_error:.4f}\")\n",
    "\n",
    "# Summary results\n",
    "print(\"\\nSummary of 20 Runs:\")\n",
    "print(f\"Mean Train Error: {np.mean(train_errors):.4f} ± {np.std(train_errors):.4f}\")\n",
    "print(f\"Mean Test Error: {np.mean(test_errors):.4f} ± {np.std(test_errors):.4f}\")\n",
    "print(f\"Mean Best c: {np.mean(best_c_values):.4f} ± {np.std(best_c_values):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5267a60d",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d6f72",
   "metadata": {},
   "source": [
    "### (b)  ‘One-versus-One’ method with the polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63fe1763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating for degree = 1\n",
      "Degree = 1: Train Error = 0.0431 ± 0.0100, Test Error = 0.0685 ± 0.0104\n",
      "Evaluating for degree = 2\n",
      "Degree = 2: Train Error = 0.0065 ± 0.0026, Test Error = 0.0384 ± 0.0053\n",
      "Evaluating for degree = 3\n",
      "Degree = 3: Train Error = 0.0028 ± 0.0013, Test Error = 0.0344 ± 0.0047\n",
      "Evaluating for degree = 4\n",
      "Degree = 4: Train Error = 0.0008 ± 0.0004, Test Error = 0.0329 ± 0.0049\n",
      "Evaluating for degree = 5\n",
      "Degree = 5: Train Error = 0.0007 ± 0.0006, Test Error = 0.0325 ± 0.0051\n",
      "Evaluating for degree = 6\n",
      "Degree = 6: Train Error = 0.0005 ± 0.0003, Test Error = 0.0336 ± 0.0042\n",
      "Evaluating for degree = 7\n",
      "Degree = 7: Train Error = 0.0003 ± 0.0002, Test Error = 0.0342 ± 0.0040\n",
      "\n",
      "Final Results:\n",
      "Degree\tTrain Error (Mean ± Std)\tTest Error (Mean ± Std)\n",
      "1\t0.0431 ± 0.0100\t\t0.0685 ± 0.0104\n",
      "2\t0.0065 ± 0.0026\t\t0.0384 ± 0.0053\n",
      "3\t0.0028 ± 0.0013\t\t0.0344 ± 0.0047\n",
      "4\t0.0008 ± 0.0004\t\t0.0329 ± 0.0049\n",
      "5\t0.0007 ± 0.0006\t\t0.0325 ± 0.0051\n",
      "6\t0.0005 ± 0.0003\t\t0.0336 ± 0.0042\n",
      "7\t0.0003 ± 0.0002\t\t0.0342 ± 0.0040\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class BinaryKernelPerceptron:\n",
    "    def __init__(self, kernel, degree, max_epochs):\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.max_epochs = max_epochs\n",
    "        self.alphas = None\n",
    "        self.support_vectors = None\n",
    "        self.support_labels = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        self.alphas = np.zeros(n_samples)\n",
    "        self.support_vectors = X\n",
    "        self.support_labels = y\n",
    "\n",
    "        kernel_matrix = self.kernel(X, X, self.degree)\n",
    "\n",
    "        for epoch in range(self.max_epochs):\n",
    "            for i in range(n_samples):\n",
    "                prediction = np.sign(np.dot(self.alphas * y, kernel_matrix[:, i]))\n",
    "                if prediction != y[i]:\n",
    "                    self.alphas[i] += 1\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        kernel_matrix = self.kernel(X_test, self.support_vectors, self.degree)\n",
    "        scores = np.dot(kernel_matrix, self.alphas * self.support_labels)\n",
    "        return np.sign(scores)\n",
    "\n",
    "class OneVersusOneKernelPerceptron:\n",
    "    def __init__(self, num_classes, kernel, degree, max_epochs):\n",
    "        self.num_classes = num_classes\n",
    "        self.kernel = kernel\n",
    "        self.degree = degree\n",
    "        self.max_epochs = max_epochs\n",
    "        self.classifiers = {}\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # Train binary classifiers for each pair of classes\n",
    "        for (class1, class2) in combinations(range(self.num_classes), 2):\n",
    "            # Filter data for the two classes\n",
    "            idx = np.where((y == class1) | (y == class2))\n",
    "            X_pair = X[idx]\n",
    "            y_pair = y[idx]\n",
    "            y_binary = np.where(y_pair == class1, 1, -1)\n",
    "\n",
    "            # Initialize binary perceptron\n",
    "            perceptron = BinaryKernelPerceptron(self.kernel, self.degree, self.max_epochs)\n",
    "            perceptron.train(X_pair, y_binary)\n",
    "            self.classifiers[(class1, class2)] = perceptron\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Perform predictions for each classifier\n",
    "        votes = np.zeros((X_test.shape[0], self.num_classes))\n",
    "        for (class1, class2), clf in self.classifiers.items():\n",
    "            binary_pred = clf.predict(X_test)\n",
    "            votes[:, class1] += (binary_pred == 1)\n",
    "            votes[:, class2] += (binary_pred == -1)\n",
    "        return np.argmax(votes, axis=1)\n",
    "\n",
    "# Parameters\n",
    "degrees = [1, 2, 3, 4, 5, 6, 7]\n",
    "max_epochs = 5\n",
    "# Arrays to store results\n",
    "results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    print(f\"Evaluating for degree = {degree}\")\n",
    "\n",
    "    # Perform 20 runs\n",
    "    for run in range(20):\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "            features, labels, test_size=0.2, random_state=run\n",
    "        )\n",
    "\n",
    "        # Train the OvO classifier\n",
    "        num_classes = len(np.unique(train_labels))\n",
    "        ovo_classifier = OneVersusOneKernelPerceptron(num_classes, polynomial_kernel, degree, max_epochs)\n",
    "        ovo_classifier.train(train_features, train_labels)\n",
    "\n",
    "        # Predict on training data\n",
    "        train_pred = ovo_classifier.predict(train_features)\n",
    "        train_error = np.mean(train_pred != train_labels)\n",
    "\n",
    "        # Predict on testing data\n",
    "        test_pred = ovo_classifier.predict(test_features)\n",
    "        test_error = np.mean(test_pred != test_labels)\n",
    "\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    # Compute mean and std for this degree\n",
    "    mean_train_error = np.mean(train_errors)\n",
    "    std_train_error = np.std(train_errors)\n",
    "    mean_test_error = np.mean(test_errors)\n",
    "    std_test_error = np.std(test_errors)\n",
    "\n",
    "    results.append((degree, mean_train_error, std_train_error, mean_test_error, std_test_error))\n",
    "\n",
    "    print(f\"Degree = {degree}: Train Error = {mean_train_error:.4f} ± {std_train_error:.4f}, \"\n",
    "          f\"Test Error = {mean_test_error:.4f} ± {std_test_error:.4f}\")\n",
    "\n",
    "# Display results in a formatted way\n",
    "print(\"\\nFinal Results:\")\n",
    "print(\"Degree\\tTrain Error (Mean ± Std)\\tTest Error (Mean ± Std)\")\n",
    "for degree, mean_train, std_train, mean_test, std_test in results:\n",
    "    print(f\"{degree}\\t{mean_train:.4f} ± {std_train:.4f}\\t\\t{mean_test:.4f} ± {std_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e07d447",
   "metadata": {},
   "source": [
    "### (c) cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42cd7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Run 1: Best Degree = 4, Train Error = 0.0003, Test Error = 0.0296\n",
      "Run 2\n",
      "Run 2: Best Degree = 5, Train Error = 0.0003, Test Error = 0.0253\n",
      "Run 3\n",
      "Run 3: Best Degree = 6, Train Error = 0.0004, Test Error = 0.0290\n",
      "Run 4\n",
      "Run 4: Best Degree = 4, Train Error = 0.0005, Test Error = 0.0425\n",
      "Run 5\n",
      "Run 5: Best Degree = 7, Train Error = 0.0005, Test Error = 0.0296\n",
      "Run 6\n",
      "Run 6: Best Degree = 4, Train Error = 0.0019, Test Error = 0.0339\n",
      "Run 7\n",
      "Run 7: Best Degree = 5, Train Error = 0.0005, Test Error = 0.0360\n",
      "Run 8\n",
      "Run 8: Best Degree = 5, Train Error = 0.0005, Test Error = 0.0312\n",
      "Run 9\n",
      "Run 9: Best Degree = 4, Train Error = 0.0012, Test Error = 0.0296\n",
      "Run 10\n",
      "Run 10: Best Degree = 5, Train Error = 0.0004, Test Error = 0.0344\n",
      "Run 11\n",
      "Run 11: Best Degree = 4, Train Error = 0.0011, Test Error = 0.0344\n",
      "Run 12\n",
      "Run 12: Best Degree = 4, Train Error = 0.0004, Test Error = 0.0301\n",
      "Run 13\n",
      "Run 13: Best Degree = 5, Train Error = 0.0015, Test Error = 0.0376\n",
      "Run 14\n",
      "Run 14: Best Degree = 5, Train Error = 0.0004, Test Error = 0.0296\n",
      "Run 15\n",
      "Run 15: Best Degree = 5, Train Error = 0.0008, Test Error = 0.0274\n",
      "Run 16\n",
      "Run 16: Best Degree = 3, Train Error = 0.0027, Test Error = 0.0285\n",
      "Run 17\n",
      "Run 17: Best Degree = 6, Train Error = 0.0003, Test Error = 0.0462\n",
      "Run 18\n",
      "Run 18: Best Degree = 4, Train Error = 0.0012, Test Error = 0.0312\n",
      "Run 19\n",
      "Run 19: Best Degree = 6, Train Error = 0.0001, Test Error = 0.0344\n",
      "Run 20\n",
      "Run 20: Best Degree = 4, Train Error = 0.0004, Test Error = 0.0344\n",
      "\n",
      "Final Results:\n",
      "Train Error: 0.0008 ± 0.0006\n",
      "Test Error: 0.0327 ± 0.0050\n",
      "Best Degree: 4.7500 ± 0.9421\n"
     ]
    }
   ],
   "source": [
    "# Arrays to store results\n",
    "final_train_errors = []\n",
    "final_test_errors = []\n",
    "final_best_degrees = []\n",
    "\n",
    "# Perform 20 runs\n",
    "for run in range(20):\n",
    "    print(f\"Run {run + 1}\")\n",
    "\n",
    "    # Split into training and test sets\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=run\n",
    "    )\n",
    "\n",
    "    best_degree = None\n",
    "    best_cv_error = float('inf')\n",
    "\n",
    "    # Cross-validation\n",
    "    for degree in degrees:\n",
    "        cv_errors = []\n",
    "        kf = KFold(n_splits=num_folds, shuffle=True, random_state=run)\n",
    "\n",
    "        for train_idx, val_idx in kf.split(train_features):\n",
    "            X_train, X_val = train_features[train_idx], train_features[val_idx]\n",
    "            y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            ovo_classifier = OneVersusOneKernelPerceptron(num_classes, polynomial_kernel, degree, max_epochs)\n",
    "            ovo_classifier.train(X_train, y_train)\n",
    "\n",
    "            val_pred = ovo_classifier.predict(X_val)\n",
    "            val_error = np.mean(val_pred != y_val)\n",
    "            cv_errors.append(val_error)\n",
    "\n",
    "        mean_cv_error = np.mean(cv_errors)\n",
    "\n",
    "        if mean_cv_error < best_cv_error:\n",
    "            best_cv_error = mean_cv_error\n",
    "            best_degree = degree\n",
    "\n",
    "    final_best_degrees.append(best_degree)\n",
    "\n",
    "    # Train on full training set with best degree\n",
    "    num_classes = len(np.unique(train_labels))\n",
    "    ovo_classifier = OneVersusOneKernelPerceptron(num_classes, polynomial_kernel, best_degree, max_epochs)\n",
    "    ovo_classifier.train(train_features, train_labels)\n",
    "\n",
    "    train_pred = ovo_classifier.predict(train_features)\n",
    "    train_error = np.mean(train_pred != train_labels)\n",
    "\n",
    "    test_pred = ovo_classifier.predict(test_features)\n",
    "    test_error = np.mean(test_pred != test_labels)\n",
    "\n",
    "    final_train_errors.append(train_error)\n",
    "    final_test_errors.append(test_error)\n",
    "\n",
    "    print(f\"Run {run + 1}: Best Degree = {best_degree}, Train Error = {train_error:.4f}, Test Error = {test_error:.4f}\")\n",
    "\n",
    "# Final results\n",
    "mean_train_error = np.mean(final_train_errors)\n",
    "std_train_error = np.std(final_train_errors)\n",
    "mean_test_error = np.mean(final_test_errors)\n",
    "std_test_error = np.std(final_test_errors)\n",
    "mean_best_degree = np.mean(final_best_degrees)\n",
    "std_best_degree = np.std(final_best_degrees)\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Train Error: {mean_train_error:.4f} ± {std_train_error:.4f}\")\n",
    "print(f\"Test Error: {mean_test_error:.4f} ± {std_test_error:.4f}\")\n",
    "print(f\"Best Degree: {mean_best_degree:.4f} ± {std_best_degree:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
